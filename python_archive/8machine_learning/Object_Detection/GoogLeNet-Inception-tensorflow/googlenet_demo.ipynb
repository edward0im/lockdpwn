{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/dyros-vehicle/gitrepo/ims/ML_archive/GoogLeNet-Inception-tensorflow'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dyros-vehicle/gitrepo/ims/ML_archive/GoogLeNet-Inception-tensorflow/example\n"
     ]
    }
   ],
   "source": [
    "cd example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# File: pre_trained.py\n",
    "# Author: Qian Ge <geqian1001@gmail.com>\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import argparse\n",
    "\n",
    "from tensorcv.dataflow.image import ImageFromFile\n",
    "\n",
    "import setup_env as conf\n",
    "from nets.googlenet import GoogleNet\n",
    "from utils.preprocess import resize_image_with_smallest_side, center_crop_image\n",
    "from utils.classes import get_word_list\n",
    "\n",
    "\n",
    "def display_data(dataflow, data_name):\n",
    "    try:\n",
    "        print('[{}] num of samples {}, num of classes {}'.\n",
    "              format(data_name, dataflow.size(), len(dataflow.label_dict)))\n",
    "    except AttributeError:\n",
    "        print('[{}] num of samples {}'.\n",
    "              format(data_name, dataflow.size()))\n",
    "    print(dataflow._im_list)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--type', default='.jpg', type=str,\n",
    "                        help='image file extension')\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_data] num of samples 3\n",
      "['../data/IMG_4379.jpg' '../data/ILSVRC2017_test_00000004.jpg'\n",
      " '../data/IMG_7940.JPG']\n",
      "**[warning]** consider use dictionary input.\n",
      "Load conv1_7x7_s2 weights!\n",
      "Load conv1_7x7_s2 biases!\n",
      "Load conv2_3x3_reduce weights!\n",
      "Load conv2_3x3_reduce biases!\n",
      "Load conv2_3x3 weights!\n",
      "Load conv2_3x3 biases!\n",
      "Load inception_3a_1x1 weights!\n",
      "Load inception_3a_1x1 biases!\n",
      "Load inception_3a_3x3_reduce weights!\n",
      "Load inception_3a_3x3_reduce biases!\n",
      "Load inception_3a_3x3 weights!\n",
      "Load inception_3a_3x3 biases!\n",
      "Load inception_3a_5x5_reduce weights!\n",
      "Load inception_3a_5x5_reduce biases!\n",
      "Load inception_3a_5x5 weights!\n",
      "Load inception_3a_5x5 biases!\n",
      "Load inception_3a_pool_proj weights!\n",
      "Load inception_3a_pool_proj biases!\n",
      "Load inception_3b_1x1 weights!\n",
      "Load inception_3b_1x1 biases!\n",
      "Load inception_3b_3x3_reduce weights!\n",
      "Load inception_3b_3x3_reduce biases!\n",
      "Load inception_3b_3x3 weights!\n",
      "Load inception_3b_3x3 biases!\n",
      "Load inception_3b_5x5_reduce weights!\n",
      "Load inception_3b_5x5_reduce biases!\n",
      "Load inception_3b_5x5 weights!\n",
      "Load inception_3b_5x5 biases!\n",
      "Load inception_3b_pool_proj weights!\n",
      "Load inception_3b_pool_proj biases!\n",
      "Load inception_4a_1x1 weights!\n",
      "Load inception_4a_1x1 biases!\n",
      "Load inception_4a_3x3_reduce weights!\n",
      "Load inception_4a_3x3_reduce biases!\n",
      "Load inception_4a_3x3 weights!\n",
      "Load inception_4a_3x3 biases!\n",
      "Load inception_4a_5x5_reduce weights!\n",
      "Load inception_4a_5x5_reduce biases!\n",
      "Load inception_4a_5x5 weights!\n",
      "Load inception_4a_5x5 biases!\n",
      "Load inception_4a_pool_proj weights!\n",
      "Load inception_4a_pool_proj biases!\n",
      "Load inception_4b_1x1 weights!\n",
      "Load inception_4b_1x1 biases!\n",
      "Load inception_4b_3x3_reduce weights!\n",
      "Load inception_4b_3x3_reduce biases!\n",
      "Load inception_4b_3x3 weights!\n",
      "Load inception_4b_3x3 biases!\n",
      "Load inception_4b_5x5_reduce weights!\n",
      "Load inception_4b_5x5_reduce biases!\n",
      "Load inception_4b_5x5 weights!\n",
      "Load inception_4b_5x5 biases!\n",
      "Load inception_4b_pool_proj weights!\n",
      "Load inception_4b_pool_proj biases!\n",
      "Load inception_4c_1x1 weights!\n",
      "Load inception_4c_1x1 biases!\n",
      "Load inception_4c_3x3_reduce weights!\n",
      "Load inception_4c_3x3_reduce biases!\n",
      "Load inception_4c_3x3 weights!\n",
      "Load inception_4c_3x3 biases!\n",
      "Load inception_4c_5x5_reduce weights!\n",
      "Load inception_4c_5x5_reduce biases!\n",
      "Load inception_4c_5x5 weights!\n",
      "Load inception_4c_5x5 biases!\n",
      "Load inception_4c_pool_proj weights!\n",
      "Load inception_4c_pool_proj biases!\n",
      "Load inception_4d_1x1 weights!\n",
      "Load inception_4d_1x1 biases!\n",
      "Load inception_4d_3x3_reduce weights!\n",
      "Load inception_4d_3x3_reduce biases!\n",
      "Load inception_4d_3x3 weights!\n",
      "Load inception_4d_3x3 biases!\n",
      "Load inception_4d_5x5_reduce weights!\n",
      "Load inception_4d_5x5_reduce biases!\n",
      "Load inception_4d_5x5 weights!\n",
      "Load inception_4d_5x5 biases!\n",
      "Load inception_4d_pool_proj weights!\n",
      "Load inception_4d_pool_proj biases!\n",
      "Load inception_4e_1x1 weights!\n",
      "Load inception_4e_1x1 biases!\n",
      "Load inception_4e_3x3_reduce weights!\n",
      "Load inception_4e_3x3_reduce biases!\n",
      "Load inception_4e_3x3 weights!\n",
      "Load inception_4e_3x3 biases!\n",
      "Load inception_4e_5x5_reduce weights!\n",
      "Load inception_4e_5x5_reduce biases!\n",
      "Load inception_4e_5x5 weights!\n",
      "Load inception_4e_5x5 biases!\n",
      "Load inception_4e_pool_proj weights!\n",
      "Load inception_4e_pool_proj biases!\n",
      "Load inception_5a_1x1 weights!\n",
      "Load inception_5a_1x1 biases!\n",
      "Load inception_5a_3x3_reduce weights!\n",
      "Load inception_5a_3x3_reduce biases!\n",
      "Load inception_5a_3x3 weights!\n",
      "Load inception_5a_3x3 biases!\n",
      "Load inception_5a_5x5_reduce weights!\n",
      "Load inception_5a_5x5_reduce biases!\n",
      "Load inception_5a_5x5 weights!\n",
      "Load inception_5a_5x5 biases!\n",
      "Load inception_5a_pool_proj weights!\n",
      "Load inception_5a_pool_proj biases!\n",
      "Load inception_5b_1x1 weights!\n",
      "Load inception_5b_1x1 biases!\n",
      "Load inception_5b_3x3_reduce weights!\n",
      "Load inception_5b_3x3_reduce biases!\n",
      "Load inception_5b_3x3 weights!\n",
      "Load inception_5b_3x3 biases!\n",
      "Load inception_5b_5x5_reduce weights!\n",
      "Load inception_5b_5x5_reduce biases!\n",
      "Load inception_5b_5x5 weights!\n",
      "Load inception_5b_5x5 biases!\n",
      "Load inception_5b_pool_proj weights!\n",
      "Load inception_5b_pool_proj biases!\n",
      "Load loss3_classifier weights!\n",
      "Load loss3_classifier biases!\n"
     ]
    }
   ],
   "source": [
    "model = GoogleNet(is_load=True, pre_train_path=conf.PARA_DIR)\n",
    "\n",
    "image = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n",
    "\n",
    "test_data = ImageFromFile(\n",
    "        # ed: FLAGS.type이 argparse라서 ipython에서 실행시키기 힘드므로 아래와 같이 한다\n",
    "        # FLAGS.type,\n",
    "        '.jpg',\n",
    "        data_dir=conf.DATA_DIR,\n",
    "        num_channel=3)\n",
    "\n",
    "display_data(test_data, 'test_data')\n",
    "word_dict = get_word_list('../data/imageNetLabel.txt')\n",
    "\n",
    "model.create_model([image, 1])\n",
    "test_op = tf.nn.top_k(tf.nn.softmax(model.layer['output']),\n",
    "                          k=5, sorted=True)\n",
    "\n",
    "input_op = model.layer['input']\n",
    "writer = tf.summary.FileWriter(conf.SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKV2(values=<tf.Tensor 'TopKV2:0' shape=(?, 5) dtype=float32>, indices=<tf.Tensor 'TopKV2:1' shape=(?, 5) dtype=int32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_prob': <tf.Tensor 'class_prob:0' shape=(?, 1000) dtype=float32>,\n",
       " 'conv_out': <tf.Tensor 'inception_5b_concat:0' shape=(?, ?, ?, 1024) dtype=float32>,\n",
       " 'input': <tf.Tensor 'Placeholder:0' shape=(?, ?, ?, 3) dtype=float32>,\n",
       " 'output': <tf.Tensor 'loss3_classifier/output:0' shape=(?, 1000) dtype=float32>,\n",
       " 'pre_prob': <tf.Tensor 'pre_prob:0' shape=(?,) dtype=float32>,\n",
       " 'prediction': <tf.Tensor 'ArgMax:0' shape=(?,) dtype=int64>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:479: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val : ', array([0.40608373, 0.24211834, 0.04435861, 0.01696582, 0.01652327],\n",
      "      dtype=float32))\n",
      "('ind : ', array([285, 281, 282, 284, 673], dtype=int32))\n",
      "('class : ', 'Egyptian cat')\n",
      "\n",
      "\n",
      "('val : ', array([9.9577928e-01, 3.1153345e-03, 7.5795321e-04, 2.2768714e-04,\n",
      "       3.9285191e-05], dtype=float32))\n",
      "('ind : ', array([  1, 392, 397,   0, 390], dtype=int32))\n",
      "('class : ', 'goldfish, Carassius auratus')\n",
      "\n",
      "\n",
      "('val : ', array([9.8797357e-01, 9.3527948e-03, 1.5818434e-03, 7.9170836e-04,\n",
      "       6.9607740e-05], dtype=float32))\n",
      "('ind : ', array([829, 705, 874, 547, 565], dtype=int32))\n",
      "('class : ', 'streetcar, tram, tramcar, trolley, trolley car')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        for k in range(0, 50):\n",
    "            if test_data.epochs_completed < 1:\n",
    "                batch_data = test_data.next_batch()\n",
    "\n",
    "                im = batch_data[0]\n",
    "                im = resize_image_with_smallest_side(im, 224)\n",
    "\n",
    "                # im = center_crop_image(im, 224, 224)\n",
    "                # scipy.misc.imsave('{}test_{}.png'.format(conf.SAVE_DIR, k),\n",
    "                #                   np.squeeze(im))\n",
    "                result = sess.run(test_op, feed_dict={image: im})\n",
    "\n",
    "                for val, ind in zip(result.values, result.indices):\n",
    "                    print('val : ' , val)\n",
    "                    print('ind : ' , ind)\n",
    "                    print('class : ' , word_dict[ind[0]])\n",
    "                    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
