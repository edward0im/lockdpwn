# 4 CNN & 3 Max pooling layer (3 SAME, 1 VALID)
# 1 fully connected layer
# drop out
# Function added for VALID option
def conv2d_valid(x, W, stride):
    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')
x_image = x

# 1st conv layer-----------------------------
W_conv1 = weight_variable([3, 3, 3, 16])
b_conv1 = bias_variable([16])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1) # (32,32) ==> (16,16)


# 2nd conv layer-----------------------------
W_conv2 = weight_variable([5,5,16,32])
b_conv2 = weight_variable([32]) 

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 1) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2) # (16,16) ==> (8,8)


# 3rd conv layer-----------------------------
W_conv3 = weight_variable([3,3,32,64])
b_conv3 = weight_variable([64])

h_conv3 = tf.nn.relu(conv2d_valid(h_pool2, W_conv3, 1) + b_conv3) # (8,8) ==> (6,6)
h_pool3 = max_pool_2x2(h_conv3) # (6,6) ==> (3,3)


# 4th conv layer-----------------------------
W_conv4 = weight_variable([4,4,64,128])
b_conv4 = weight_variable([128])

h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4, 1) + b_conv4) # (3,3) ==> (3,3)


# 1st Fully Connected layer------------------------
n1, n2, n3, n4 = h_conv4.get_shape().as_list()

W_fc1 = weight_variable([n2*n3*n4, 3])
b_fc1 = bias_variable([3])

h_pool1_flat = tf.reshape(h_conv4, [-1, n2*n3*n4]) 


y= tf.matmul(h_pool1_flat, W_fc1) + b_fc1 # (3,3) ==> (3,1)


saver = tf.train.Saver({'W_conv1' : W_conv1, 'b_conv1' : b_conv1, 'W_conv2' : W_conv2,'b_conv2' : b_conv2, 'W_conv3' : W_conv3, 'b_conv3' : b_conv3, 'W_conv4' : W_conv4,'b_conv4' : b_conv4, 'W_fc1' : W_fc1, 'b_fc1' : b_fc1})

