# 3 CNN & 3 Max pooling layer (3 SAME)
# 1 fully connected layer
# drop out
x_image = x

# 1st conv layer-----------------------------
W_conv1 = weight_variable([3, 3, 3, 16])
b_conv1 = bias_variable([16])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1) # (32,32) ==> (16,16)


# 2nd conv layer-----------------------------
W_conv2 = weight_variable([5,5,16,32])
b_conv2 = weight_variable([32]) 

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 1) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2) # (16,16) ==> (8,8)


# 3rd conv layer-----------------------------
W_conv3 = weight_variable([3,3,32,64])
b_conv3 = weight_variable([64])

h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3, 1) + b_conv3) # (8,8) ==> (8,8)
h_pool3 = max_pool_2x2(h_conv3) # (8,8) ==> (4,4)


# 1st Fully Connected layer------------------------
n1, n2, n3, n4 = h_pool3.get_shape().as_list()

W_fc1 = weight_variable([n2*n3*n4, 3])
b_fc1 = bias_variable([3])

h_pool1_flat = tf.reshape(h_pool3, [-1, n2*n3*n4]) 


y= tf.matmul(h_pool1_flat, W_fc1) + b_fc1 # (4,4) ==> (3,1)


saver = tf.train.Saver({'W_conv1' : W_conv1, 'b_conv1' : b_conv1, 'W_conv2' : W_conv2,'b_conv2' : b_conv2, 'W_conv3' : W_conv3, 'b_conv3' : b_conv3,  'W_fc1' : W_fc1, 'b_fc1' : b_fc1})

